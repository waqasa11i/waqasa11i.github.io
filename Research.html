<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research - Waqas Ali</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }
        .header {
            background-color: #007BFF;
            color: white;
            text-align: center;
            padding: 20px;
        }
        .container {
            width: 80%;
            margin: 0 auto;
            padding: 20px;
        }
        .project {
            display: flex;
            flex-direction: column;
            margin-bottom: 30px;
            background-color: white;
            padding: 15px;
            border-radius: 10px;
            box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
        }
        .project img {
            width: 200px;
            height: auto;
            border-radius: 10px;
            margin-right: 20px;
        }
        .project-info {
            flex: 1;
        }
        .project h2 {
            font-size: 24px;
            margin-bottom: 10px;
        }
        .project p {
            font-size: 16px;
            line-height: 1.6;
        }
        .project a {
            color: #007BFF;
            text-decoration: none;
            font-weight: bold;
        }
        .project a:hover {
            text-decoration: underline;
        }

        /* Sub-section styles with images */
        .sub-section {
            display: flex;
            align-items: center;
            margin-top: 20px;
            padding: 15px;
            background: #f9f9f9;
            border-radius: 8px;
        }
        .sub-section img {
            width: 150px;
            height: auto;
            border-radius: 8px;
            margin-right: 20px;
        }
        .sub-section-text {
            flex: 1;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .project {
                flex-direction: column;
            }
            .sub-section {
                flex-direction: column;
                text-align: center;
            }
            .sub-section img {
                margin-bottom: 15px;
            }
        }
    </style>
</head>
<body>

    <div class="header">
        <h1>Research - Waqas Ali</h1>
    </div>

    <div class="container">
        <!-- Project 1 -->
        <div class="project">
            <h2>CLASLAM (Complex Large Scale Localization and Mapping)</h2>
            <p><strong>Funding:</strong> Wallenberg AI, Autonomous Systems and Software Program</p>
            <p>Description: Research and development of robust localization and mapping capabilities in complex large-scale environments with consumer-grade LiDARs and minimal human intervention, directly supporting downstream tasks.</p>

            <!-- Sub-section 1 -->
            <div class="sub-section">
                <!-- <img src="lidar_processing.jpg" alt="LiDAR Data Processing"> -->
                <div class="sub-section-text">
                    <h3>Loop Closure Detection for Large scale environments</h3>
                    <p>Loop closure detection for large scale environments.</p>
                </div>
            </div>

            <!-- Sub-section 2 -->
            <div class="sub-section">
                <img src="assets/ITSC_paper.png" alt="ITSC_paper">
                <div class="sub-section-text">
                    <h3>HD-maps as prior information for globally consistent mapping - [<a href="https://arxiv.org/pdf/2407.19463" target="_blank">link</a>] </h3>
                    <p> We proposed a lidar based localization and mapping (LOAM) system that can  exploit the common HD-maps in autonomous driving scenarios.  Specifically, we propose a technique to extract information from  the drivable area and ground surface height components of the HD-maps to construct 4DOF pose priors. These pose priors are then further integrated into the pose-graph optimization problem to create a globally consistent 3D map.</p>

                </div>
            </div>
        </div>

        <!-- Project 2 -->
        <div class="project">
            <img src="assets/ll_slam.jpg" alt="ll_slam_Image">
            <div class="project-info">
                <h2>Life-long SLAM using 3D LiDAR</h2>
                <p><strong>Organization:</strong> Shanghai Key Lab of Navigation and Location-based Services, Shanghai Jiaotong University</p>
                <p>Description: Most real-time autonomous robot applications require a robot to traverse through a dynamic space for a long time. In some cases, a robot needs to work in the same environment. Such applications give rise to the problem of a life-long SLAM system. Life-long SLAM presents two main challenges i.e. the tracking should not fail in a dynamic environment and the need for a robust and efficient mapping strategy. The system should update maps with new information; while also keeping track of older observations. But, mapping for a long time can require higher computational requirements. In this project, we proposed a solution to the problem of life-long SLAM. We represent the global map as a set of rasterized images of local maps along with a map management system responsible for updating local maps and keeping track of older values. We also present an efficient approach of using the bag of visual words method for loop closure detection and relocalization. We evaluate the performance of our system on the KITTI dataset and an indoor dataset. Our loop closure system reported recall and precision of above 90 percent. The computational cost of our system is much lower as compared to state-of-the-art methods. Our method reports lower computational requirements even for long-term operation. The work was published in the IEEE Sensors Journal.</p>
            </div>
        </div>

        <!-- Project 3 -->
        <div class="project">
            <img src="assets/UAV_swarm.jpg" alt="UAV_swarm">
            <div class="project-info">
                <h2>Fixed-wing UAV Swarm</h2>
                <p><strong>Funding:</strong> China Electronics Technology Group Corporation</p>
                <p>Description: A swarm of 119 small fixed-wing UAVs successfully demonstrated actions such as intensive ejection take-off, aerial assembly, multi-target grouping, formation encirclement, and group operations. As a system engineer, I contributed to the implementation of the leader-follower swarm formation with collision avoidance among swarm agents.</p>
            </div>
        </div>
    </div>

</body>
</html>
