<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects - Waqas Ali</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }
        .header {
            background-color: #007BFF;
            color: white;
            text-align: center;
            padding: 20px;
        }
        .container {
            width: 80%;
            margin: 0 auto;
            padding: 20px;
        }
        .project {
            display: flex;
            flex-direction: row;
            align-items: flex-start;
            margin-bottom: 30px;
            background-color: white;
            padding: 15px;
            border-radius: 10px;
            box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
        }
        .project img {
            width: 200px;
            height: auto;
            border-radius: 10px;
            margin-right: 20px;
        }
        .project-info {
            flex: 1;
        }
        .project h2 {
            font-size: 24px;
            margin-bottom: 10px;
        }
        .project p {
            font-size: 16px;
            line-height: 1.6;
        }
        .project a {
            color: #007BFF;
            text-decoration: none;
            font-weight: bold;
        }
        .project a:hover {
            text-decoration: underline;
        }

        /* Sub-section styles with images */
        .sub-section {
            display: flex;
            align-items: flex-start;
            margin-top: 20px;
            padding: 15px;
            background: #f9f9f9;
            border-radius: 8px;
        }
        .sub-section img {
            width: 150px;
            height: auto;
            border-radius: 8px;
            margin-right: 20px;
        }
        .sub-section-text {
            flex: 1;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .project {
                flex-direction: column;
            }
            .sub-section {
                flex-direction: column;
                text-align: center;
            }
            .sub-section img {
                margin-bottom: 15px;
            }
        }
    </style>
</head>
<body>

    <div class="header">
        <h1>Projects - Waqas Ali</h1>
    </div>

    <div class="container">
        <!-- Project 1 -->
        <div class="project">
            <img src="assets/CLaSLAM.png" alt="ITSC_paper">
            <div class="project-info">
                <h2>CLASLAM (Complex Large Scale Localization and Mapping)</h2>
                <p><strong>Funding:</strong> Wallenberg AI, Autonomous Systems and Software Program</p>
                <p><strong>Organization:</strong> Division of Robotics, Perception and Learning, KTH Royal Institute of Technology</p>
                <p>Description: Research and development of robust localization and mapping capabilities in complex large-scale environments with consumer-grade LiDARs and minimal human intervention, directly supporting downstream tasks.</p>

                <!-- Sub-section 1 -->
                <div class="sub-section">
                    <img src="assets/svtloop.png" alt="SVTLOOP">
                    <div class="sub-section-text">
                        <h3>Loop Closure Detection for Large scale environments - [<a href="https://github.com/KTH-RPL/awesome-svtloop" target="_blank">link</a>]</h3>
                        <p>Loop closure detection for large scale environments.</p>
                    </div>
                </div>

                <!-- Sub-section 2 -->
                <div class="sub-section">
                    <img src="assets/semantics.png" alt="Semantics">
                    <div class="sub-section-text">
                        <h3>Semantic annotations of MCD Dataset</h3>
                        <p>The project involved semantic annotations of Ouster point cloud provided in the MCD dataset levergaing the ground truth labels provided for Livox point cloud.</p>
                    </div>
                </div>

                <!-- Sub-section 3 -->
                <div class="sub-section">
                    <img src="assets/ITSC_paper.png" alt="ITSC_paper">
                    <div class="sub-section-text">
                        <h3>HD-maps as prior information for globally consistent mapping - [<a href="https://arxiv.org/pdf/2407.19463" target="_blank">link</a>] </h3>
                        <p>We proposed a lidar-based localization and mapping (LOAM) system that can exploit the common HD-maps in autonomous driving scenarios. Specifically, we propose a technique to extract information from the drivable area and ground surface height components of the HD-maps to construct 4DOF pose priors. These pose priors are then further integrated into the pose-graph optimization problem to create a globally consistent 3D map.</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Project 2 -->
        <div class="project">
            <img src="assets/Pluto.jpg" alt="Pluto">
            <div class="project-info">
                <h2>Autonomous Navigation System for PLUTO ATV</h2>
                <p><strong>Organization:</strong> Division of Robotics, Perception and Learning, KTH Royal Institute of Technology</p>
                <p>Description: I supervised the development of PLUTO All-Terrain Vehicle (ATV) as a fully autonomous research platform at KTH Royal Institute of Technology, within the Division of Robotics, Perception, and Learning. The primary goal of the project was to enable autonomous navigation of the ATV across the KTH campus for real-time data collection and algorithm benchmarking. The system integrates localization, mapping, perception, path planning, and control to achieve fully autonomous operation in large-scale outdoor environments. The autonomous system was successfully tested on campus, demonstrating its ability to navigate complex environments while collecting high-quality sensor data. Future work will focus on benchmarking and optimizing localization, planning, and perception modules to enhance efficiency, robustness, and real-time performance.</p>
            </div>
        </div>

        <!-- Project 3 -->
        <div class="project">
            <img src="assets/sweeper.jpeg" alt="Street_sweeper">
            <div class="project-info">
                <h2>Autonomous Street Sweeper</h2>
                <p><strong>Organization:</strong> Infore Enviro, Shenzhen China</p>
                <p>Description: The project aimed at developing autonomous street sweepers with Level 4 autonomy for operation in urban environments. As a Senior SLAM Algorithm Engineer, I was responsible for designing and implementing the mapping and localization modules for the system. For the mapping module, I employed a factor-graph-based approach to generate accurate and scalable 3D maps of large-scale environments, leveraging data from LiDAR, GPS, and IMU sensors. In the localization module, I implemented an Extended State Kalman Filter (ESKF) to fuse LiDAR odometry with GPS and IMU data, ensuring robust and precise localization in dynamic environments. The project was successfully completed, and the autonomous street sweepers are now operational at multiple locations across China.</p>
            </div>
        </div>

        <!-- Project 4 -->
        <div class="project">
            <img src="assets/ll_slam.jpg" alt="ll_slam_Image">
            <div class="project-info">
                <h2>Life-long SLAM using 3D LiDAR</h2>
                <p><strong>Organization:</strong> Shanghai Key Lab of Navigation and Location-based Services, Shanghai Jiaotong University</p>
                <p>Description: In this project, we proposed a solution to the problem of life-long SLAM. We represent the global map as a set of rasterized images of local maps along with a map management system responsible for updating local maps and keeping track of older values. We also present an efficient approach of using the bag of visual words method for loop closure detection and relocalization. We evaluate the performance of our system on the KITTI dataset and an indoor dataset. Our loop closure system reported recall and precision of above 90 percent. The computational cost of our system is much lower as compared to state-of-the-art methods. Our method reports lower computational requirements even for long-term operation. The work was published in the IEEE Sensors Journal.</p>
            </div>
        </div>

        <!-- Project 5 -->
        <div class="project">
            <img src="assets/UAV_swarm.png" alt="UAV_swarm">
            <div class="project-info">
                <h2>Fixed-wing UAV Swarm</h2>
                <p><strong>Funding:</strong> China Electronics Technology Group Corporation</p>
                <p>Description: A swarm of 119 small fixed-wing UAVs successfully demonstrated actions such as intensive ejection take-off, aerial assembly, multi-target grouping, formation encirclement, and group operations. As a system engineer, I contributed to the implementation of the leader-follower swarm formation with collision avoidance among swarm agents.</p>
            </div>
        </div>
    </div>

</body>
</html>
